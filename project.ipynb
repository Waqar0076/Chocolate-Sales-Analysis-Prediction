{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas\n",
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy\n",
    "pip install matplotlib\n",
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import pandas as pd\n",
    "\n",
    "    # Load the Excel file\n",
    "    file_path = \"Chocolate Sales.xlsx\"  # Update the path if needed\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "\n",
    "    # Load the data from the first sheet\n",
    "    df = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "\n",
    "    # Get data structure details\n",
    "    num_rows, num_columns = df.shape\n",
    "    column_names = df.columns.tolist()\n",
    "\n",
    "    print(f\"Rows: {num_rows}\")\n",
    "    print(f\"Columns: {num_columns}\")\n",
    "    print(f\"Column Names: {column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Fill missing values\n",
    "df_filled = df.fillna(method='ffill')  # Forward fill\n",
    "\n",
    "# Verify if missing values are filled\n",
    "missing_values_after = df_filled.isnull().sum()\n",
    "\n",
    "missing_values, missing_values_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates using pandas query-like syntax\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a CSV file\n",
    "df.to_csv(\"Chocolate_Sales_Cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics for all categorical (qualitative) variables\n",
    "qualitative_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# For each qualitative variable, display frequency counts and mode\n",
    "for col in qualitative_columns:\n",
    "    print(f\"Descriptive statistics for {col}:\")\n",
    "    print(\"Frequency Counts:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(\"Mode:\")\n",
    "    print(df[col].mode()[0])  # Mode returns a Series, so we take the first element\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select quantitative (numerical) columns\n",
    "quantitative_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate the correlation matrix between quantitative variables\n",
    "correlation_matrix = df[quantitative_columns].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(\"Correlation matrix between quantitative variables:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Chocolate Sales.xlsx\"  # Update the path if needed\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the data from the first sheet\n",
    "df = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "\n",
    "# Get qualitative (categorical) columns\n",
    "qualitative_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "variable1 = 'Product Type'  \n",
    "variable2 = 'Region'        \n",
    "\n",
    "# Create a contingency table (cross-tabulation) of the two categorical variables\n",
    "contingency_table = pd.crosstab(df[variable1], df[variable2])\n",
    "\n",
    "# Perform the Chi-square test of independence\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Chi-square statistic: {chi2}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"P-value: {p}\")\n",
    "print(f\"Expected frequencies table:\\n{expected}\")\n",
    "\n",
    "# Interpret the result\n",
    "if p < 0.05:\n",
    "    print(f\"There's a significant association between {variable1} and {variable2} (p-value < 0.05).\")\n",
    "else:\n",
    "    print(f\"There's no significant association between {variable1} and {variable2} (p-value >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Chocolate Sales.xlsx\"  # Update the path if needed\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the data from the first sheet\n",
    "df = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "\n",
    "# 1. Data Overview\n",
    "print(\"Data Overview:\")\n",
    "print(df.info())  # Basic info (data types, non-null counts)\n",
    "print(\"\\nFirst 5 rows of data:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Missing Data Analysis\n",
    "print(\"\\nMissing Data Analysis:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(df)) * 100\n",
    "missing_summary = pd.DataFrame({'Missing Values': missing_data, 'Percentage': missing_percentage})\n",
    "print(missing_summary[missing_summary['Missing Values'] > 0])  # Only show columns with missing values\n",
    "\n",
    "# 3. Summary Statistics for Quantitative Variables\n",
    "print(\"\\nSummary Statistics for Quantitative Variables:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 4. Distribution of Quantitative Variables (Visual)\n",
    "quantitative_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Plot histograms for each quantitative column\n",
    "for col in quantitative_columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df[col].hist(bins=20, color='lightblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# 5. Correlation Analysis for Quantitative Variables\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "correlation_matrix = df[quantitative_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap for Quantitative Variables')\n",
    "plt.show()\n",
    "\n",
    "# 6. Categorical Data Insights\n",
    "qualitative_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Frequency of each category in categorical columns\n",
    "print(\"\\nCategorical Data Frequency Counts:\")\n",
    "for col in qualitative_columns:\n",
    "    print(f\"\\n{col} Value Counts:\")\n",
    "    print(df[col].value_counts())\n",
    "\n",
    "# Bar plot for each categorical variable\n",
    "for col in qualitative_columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    df[col].value_counts().plot(kind='bar', color='skyblue')\n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# 7. Outlier Detection (For Quantitative Variables)\n",
    "print(\"\\nOutlier Detection (using IQR method):\")\n",
    "for col in quantitative_columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"\\nOutliers for {col}:\")\n",
    "    print(outliers[col])\n",
    "\n",
    "# 8. Pairplot (Visualizing Relationships Between Quantitative Variables)\n",
    "sns.pairplot(df[quantitative_columns])\n",
    "plt.title('Pairplot of Quantitative Variables')\n",
    "plt.show()\n",
    "\n",
    "# 9. Boxplots to detect outliers for each quantitative variable\n",
    "for col in quantitative_columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Chocolate Sales.xlsx\"  # Update the path if needed\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the data from the first sheet\n",
    "df = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "\n",
    "# 1. Handle Missing Data\n",
    "# For numerical features, we can use the median to fill missing values\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[numerical_columns] = imputer.fit_transform(df[numerical_columns])\n",
    "\n",
    "# For categorical features, we can use the most frequent value to fill missing values\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_columns] = imputer_cat.fit_transform(df[categorical_columns])\n",
    "\n",
    "if 'Quantity' in df.columns and 'Price' in df.columns:\n",
    "    df['Revenue'] = df['Quantity'] * df['Price']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "target = 'Sales'  # Replace with the name of your target variable\n",
    "features = df.drop(columns=[target])\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the final data\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Test features shape:\", X_test.shape)\n",
    "print(\"Training target shape:\", y_train.shape)\n",
    "print(\"Test target shape:\", y_test.shape)\n",
    "\n",
    "for col in numerical_columns:\n",
    "    Q1 = X_train[col].quantile(0.25)\n",
    "    Q3 = X_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    X_train = X_train[(X_train[col] >= lower_bound) & (X_train[col] <= upper_bound)]\n",
    "    y_train = y_train[X_train.index]  # Ensure y_train matches the filtered X_train\n",
    "\n",
    "# Check the final processed data\n",
    "print(\"Processed training features shape:\", X_train.shape)\n",
    "print(\"Processed test features shape:\", X_test.shape)\n",
    "\n",
    "# The data is now ready for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the RandomForest Regressor model\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize the RandomForest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Regressor - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest Regressor - R-squared: {r2_rf}\")\n",
    "\n",
    "# Feature importances\n",
    "feature_importances_rf = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "for feature, importance in zip(feature_names, feature_importances_rf):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33mTotal_Value\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mQuantity\u001b[39m\u001b[33m'\u001b[39m] + df[\u001b[33m'\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Add this new feature to the training and testing data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train[\u001b[33m'\u001b[39m\u001b[33mTotal_Value\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mX_train\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mQuantity\u001b[39m\u001b[33m'\u001b[39m] + X_train[\u001b[33m'\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m X_test[\u001b[33m'\u001b[39m\u001b[33mTotal_Value\u001b[39m\u001b[33m'\u001b[39m] = X_test[\u001b[33m'\u001b[39m\u001b[33mQuantity\u001b[39m\u001b[33m'\u001b[39m] + X_test[\u001b[33m'\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Re-train the RandomForest model with the new feature included\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a new feature 'Total_Value' as the sum of 'Quantity' and 'Price'\n",
    "if 'Quantity' in df.columns and 'Price' in df.columns:\n",
    "    df['Total_Value'] = df['Quantity'] + df['Price']\n",
    "\n",
    "# Add this new feature to the training and testing data\n",
    "X_train['Total_Value'] = X_train['Quantity'] + X_train['Price']\n",
    "X_test['Total_Value'] = X_test['Quantity'] + X_test['Price']\n",
    "\n",
    "# Re-train the RandomForest model with the new feature included\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate as before\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Regressor with Total_Value - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest Regressor with Total_Value - R-squared: {r2_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create a binary target variable (1 if Sales > 500, else 0)\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Create a binary target variable (1 if Sales > 500, else 0)\n",
    "y_binary = (df[target] > 500).astype(int)  # Change 500 to whatever threshold you prefer\n",
    "\n",
    "# Split the data into training and testing sets for binary classification\n",
    "X_train, X_test, y_train_binary, y_test_binary = train_test_split(features, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "logreg_model = LogisticRegression(random_state=42)\n",
    "logreg_model.fit(X_train, y_train_binary)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_logreg)\n",
    "conf_matrix = confusion_matrix(y_test_binary, y_pred_logreg)\n",
    "\n",
    "print(f\"Logistic Regression - Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
